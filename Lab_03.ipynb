{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIMth9+k+80uQE+RNeEVMJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarriRohan/Reinforcement-Learning-2025/blob/main/Lab_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W25JxPIxtqkD",
        "outputId": "eb374080-c3e6-44ca-aad5-6285da02f97b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10000/500000\n",
            "Episode 20000/500000\n",
            "Episode 30000/500000\n",
            "Episode 40000/500000\n",
            "Episode 50000/500000\n",
            "Episode 60000/500000\n",
            "Episode 70000/500000\n",
            "Episode 80000/500000\n",
            "Episode 90000/500000\n",
            "Episode 100000/500000\n",
            "Episode 110000/500000\n",
            "Episode 120000/500000\n",
            "Episode 130000/500000\n",
            "Episode 140000/500000\n",
            "Episode 150000/500000\n",
            "Episode 160000/500000\n",
            "Episode 170000/500000\n",
            "Episode 180000/500000\n",
            "Episode 190000/500000\n",
            "Episode 200000/500000\n",
            "Episode 210000/500000\n",
            "Episode 220000/500000\n",
            "Episode 230000/500000\n",
            "Episode 240000/500000\n",
            "Episode 250000/500000\n",
            "Episode 260000/500000\n",
            "Episode 270000/500000\n",
            "Episode 280000/500000\n",
            "Episode 290000/500000\n",
            "Episode 300000/500000\n",
            "Episode 310000/500000\n",
            "Episode 320000/500000\n",
            "Episode 330000/500000\n",
            "Episode 340000/500000\n",
            "Episode 350000/500000\n",
            "Episode 360000/500000\n",
            "Episode 370000/500000\n",
            "Episode 380000/500000\n",
            "Episode 390000/500000\n",
            "Episode 400000/500000\n",
            "Episode 410000/500000\n",
            "Episode 420000/500000\n",
            "Episode 430000/500000\n",
            "Episode 440000/500000\n",
            "Episode 450000/500000\n",
            "Episode 460000/500000\n",
            "Episode 470000/500000\n",
            "Episode 480000/500000\n",
            "Episode 490000/500000\n",
            "Episode 500000/500000\n",
            "Q-values for state (20, 10, False) : [ 0.42853807 -0.88190314]\n"
          ]
        }
      ],
      "source": [
        "#Monte Carlo Policy Evaluation & Control (ε-greedy) – Blackjack Example\n",
        "# mc_blackjack.py\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def make_epsilon_greedy_policy(Q: Dict, nA: int, epsilon: float):\n",
        "    \"\"\"Return a policy function that takes state and returns action probabilities.\"\"\"\n",
        "    def policy_fn(state):\n",
        "        probs = np.ones(nA) * (epsilon / nA)\n",
        "        q_vals = Q[state]\n",
        "        best_a = np.argmax(q_vals)\n",
        "        probs[best_a] += (1.0 - epsilon)\n",
        "        return probs\n",
        "    return policy_fn\n",
        "\n",
        "def generate_episode(env, policy):\n",
        "    \"\"\"Generate an episode: returns list of (state, action, reward). Uses policy as action-prob function.\"\"\"\n",
        "    episode = []\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        probs = policy(state)\n",
        "        action = np.random.choice(len(probs), p=probs)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "    return episode\n",
        "\n",
        "# ---------- First-Visit MC Policy Evaluation ----------\n",
        "def first_visit_mc_policy_evaluation(env, policy_fn, gamma=1.0, num_episodes=10000):\n",
        "    \"\"\"\n",
        "    Estimate V(s) for the given policy (state-values) with first-visit MC.\n",
        "    env: gymnasium env\n",
        "    policy_fn: function state -> action-probabilities\n",
        "    gamma: discount factor\n",
        "    num_episodes: number of episodes to generate\n",
        "    \"\"\" # Added closing triple quote\n",
        "    # Implement First-Visit MC Policy Evaluation logic here\n",
        "    # (This part of the code is missing, but the syntax error is in the docstring)\n",
        "    pass # Placeholder, as the function body was not provided in the original error.\n",
        "\n",
        "# Note: The rest of the code for Monte Carlo Control was not part of the provided error snippet,\n",
        "# so it remains as it was in the original cell for now. If there are further errors,\n",
        "# please provide the complete code.\n",
        "\n",
        "# Use the new step API by setting new_step_api=True\n",
        "env = gym.make(\"Blackjack-v1\", sab=True) # Removed new_step_api=True\n",
        "\n",
        "# ε-greedy policy\n",
        "def epsilon_greedy_policy(Q, state, nA, epsilon): # Added epsilon parameter\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(nA)\n",
        "    else:\n",
        "        return np.argmax(Q[state])\n",
        "\n",
        "# Generate an episode following policy π\n",
        "def generate_episode(Q, epsilon, nA):\n",
        "    episode = []\n",
        "    state, _ = env.reset() # Reset returns a tuple in the new API\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = epsilon_greedy_policy(Q, state, nA, epsilon) # Pass epsilon to epsilon_greedy_policy\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action) # New API returns terminated and truncated\n",
        "        done = terminated or truncated # Done is now the logical OR of terminated and truncated\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "    return episode\n",
        "\n",
        "# Monte Carlo Control with ε-greedy\n",
        "def mc_control_epsilon_greedy(num_episodes=500000, gamma=1.0, epsilon=0.1):\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))  # Q[state][action]\n",
        "    returns_sum = defaultdict(float)\n",
        "    returns_count = defaultdict(float)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        episode = generate_episode(Q, epsilon, env.action_space.n)\n",
        "        G = 0\n",
        "        visited = set()\n",
        "\n",
        "        # Work backwards through episode\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = gamma * G + reward\n",
        "\n",
        "            if (state, action) not in visited:\n",
        "                visited.add((state, action))\n",
        "                returns_sum[(state, action)] += G\n",
        "                returns_count[(state, action)] += 1\n",
        "                Q[state][action] = returns_sum[(state, action)] / returns_count[(state, action)]\n",
        "\n",
        "        # Optional: print progress\n",
        "        if i_episode % 10000 == 0: # Increased print frequency\n",
        "            print(f\"Episode {i_episode}/{num_episodes}\")\n",
        "\n",
        "    policy = {s: np.argmax(a) for s, a in Q.items()}\n",
        "    return policy, Q\n",
        "\n",
        "# Run the MC Control\n",
        "final_policy, final_Q = mc_control_epsilon_greedy()\n",
        "\n",
        "# Example: View Q-value for a specific state\n",
        "sample_state = (20, 10, False)  # (player_sum, dealer_card, usable_ace)\n",
        "print(\"Q-values for state\", sample_state, \":\", final_Q[sample_state])"
      ]
    }
  ]
}